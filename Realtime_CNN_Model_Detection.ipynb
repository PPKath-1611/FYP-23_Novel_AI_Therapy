{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Realtime Depression Detection Model using CNN and realtime images"
      ],
      "metadata": {
        "id": "qnsDBhqu41fG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Depression has been a common syndrome for most of the people in today's fast growing generation. So to spread awareness and to grow intellectuality in this domain, we are trying here to prototype a model which gives us the status and level of depression a person is facing by getting a realtime video streaming, capturing the patient's facial expressions and extracting out features from the set of images and process the set of realtime images through our CNN model to classify these into different categories such as **Happy, Sad, Fear, Disgust, Surprise, Neutral, Angry.** \n",
        "#### Post classification, we are trying to implement a logic that extracts the percentage of images falling under all of the above mentioned categories and generating a logic from those percentages and converting the values into a linear scale and classifying the results into 5 types of studied depressions mentioned below:\n",
        "\n",
        "\n",
        "*   **Minimal Depression**\n",
        "*   **Mild Depression**\n",
        "*   **Moderate Depression**\n",
        "*   **Moderately Severe Depression**\n",
        "\n",
        "*   **Severe Depression**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ncl5Hm1u5Bdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing libraries"
      ],
      "metadata": {
        "id": "jtreAuc57Zuy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFXnDwmVD12t"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "import glob\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting files from zip arhives"
      ],
      "metadata": {
        "id": "FipnszZK7rD3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3SNNTRvH_5G",
        "outputId": "bf944f1c-1e36-4223-ac08-dbe8569855ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "file_name = 'Image.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading sample image from training dataset"
      ],
      "metadata": {
        "id": "2Esg_2TM7xVd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kdcb4tT0O04y"
      },
      "outputs": [],
      "source": [
        "img_array = cv2.imread('/content/Train/angry/Training_10118481.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting size and dimension of RGB images"
      ],
      "metadata": {
        "id": "jBxgrAuP72jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_array.shape  # These images are RGB images"
      ],
      "metadata": {
        "id": "FbGkfxOF_zOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### training dataset directory"
      ],
      "metadata": {
        "id": "KkXRj3kr78uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data_directory = \"/content/Train\"  # Training Dataset"
      ],
      "metadata": {
        "id": "ivh3wabUBOzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Labels or Classes that will be used to classify the images by CNN model"
      ],
      "metadata": {
        "id": "whCCvoXW7_XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"] # These classes should be folder names"
      ],
      "metadata": {
        "id": "Vye_KxG8Bqqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading and displaying the images"
      ],
      "metadata": {
        "id": "oZSKG1Wv8IzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for category in classes:\n",
        "  path = os.path.join(Data_directory, category)\n",
        "  for img in os.listdir(path):\n",
        "    img_array = cv2.imread(os.path.join(path, img))\n",
        "    plt.imshow(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))\n",
        "    plt.show()\n",
        "    break\n",
        "  break"
      ],
      "metadata": {
        "id": "c73UNJtXDMuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preffered size of image for our CNN model"
      ],
      "metadata": {
        "id": "6MtrCM4M8h4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = 224  # ImageNet = 224 x 224"
      ],
      "metadata": {
        "id": "5MO6TxJcDdSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resizing the realtime images to preferred image size"
      ],
      "metadata": {
        "id": "wp8pyWH58oEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_array = cv2.resize(img_array, (img_size, img_size))\n",
        "plt.imshow(cv2.cvtColor(new_array, cv2.COLOR_BGR2RGB))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KmN6Kx-AD2lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confirming shape of the images"
      ],
      "metadata": {
        "id": "BcaBKB858-PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_array.shape"
      ],
      "metadata": {
        "id": "xbxskLGOEewt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read all the images and convert them into array"
      ],
      "metadata": {
        "id": "-ebfGc4aEt85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = []  ## Training Data\n",
        "\n",
        "def create_training_data():\n",
        "  for category in classes:\n",
        "    path = os.path.join(Data_directory, category)\n",
        "    class_num = classes.index(category)\n",
        "    for img in os.listdir(path):\n",
        "      try:\n",
        "        img_array = cv2.imread(os.path.join(path, img))\n",
        "        new_array = cv2.resize(img_array, (img_size, img_size))\n",
        "        training_data.append([new_array, class_num])\n",
        "      except Exception as e:\n",
        "        pass"
      ],
      "metadata": {
        "id": "UbGPYp6_EwoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Training Dataset for CNN Model"
      ],
      "metadata": {
        "id": "FKX8xOCF9DYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "create_training_data()"
      ],
      "metadata": {
        "id": "CmVY35GBGnqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Randomly shuffling the images in training dataset"
      ],
      "metadata": {
        "id": "Qcv-KyR59k7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(training_data)"
      ],
      "metadata": {
        "id": "45CezUfhHZ-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting and assigning the features and lebels to the images present in the dataset into two different arrays"
      ],
      "metadata": {
        "id": "zLFFikZ19zFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = [] # data/feature\n",
        "Y = [] # label\n",
        "\n",
        "for feature, label in training_data:\n",
        "  X.append(feature)\n",
        "  Y.append(label)\n",
        "\n",
        "X = np.array(X).reshape(-1, img_size, img_size, 3)  # Converting it to 4 dimension"
      ],
      "metadata": {
        "id": "ZWlR7RpUHduY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "uqD74QeKH-L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the data\n",
        "X = X / 255.0   # We are normalizing it into image pixels"
      ],
      "metadata": {
        "id": "694wcLrZIoh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = np.array(Y)"
      ],
      "metadata": {
        "id": "A1n68aILJM6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y.shape"
      ],
      "metadata": {
        "id": "i4PxhU9GJ1NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Learning Model for Training\n",
        "\n",
        "#### Implementing a nested layer of CNN and MaxPooling layers\n",
        "#### Using maximum 96 filters to extract and classify features from the images of training dataset to get more clear insights"
      ],
      "metadata": {
        "id": "NIDuoILMKK9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = models.Sequential([\n",
        "    layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    \n",
        "    layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),     # Creating a pattern of CNN and MaxPooling layers to reduce feature dimensionality as it reduces the computational cost by reducing the number of parameters to learn\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(filters=96, kernel_size=(3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    \n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "NlOWP80tu1dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debugging the code before running the model"
      ],
      "metadata": {
        "id": "fomnNa7I-xs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "aUi740lLvK4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the model for 10 epochs"
      ],
      "metadata": {
        "id": "ytM0_TVp-8HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.fit(X, Y, epochs=10)"
      ],
      "metadata": {
        "id": "Rc1aqBHzvRT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We are securing 96.51% of accuracy for our CNN model"
      ],
      "metadata": {
        "id": "0rNrhuPVDGY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.summary()"
      ],
      "metadata": {
        "id": "JSHG8nYTNR72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving our CNN model "
      ],
      "metadata": {
        "id": "67Wf87eYLCwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.save('Face_CNN_Model.h5')"
      ],
      "metadata": {
        "id": "mXsAyJ3iLLDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the saved CNN model"
      ],
      "metadata": {
        "id": "93qPndYL_MCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = tf.keras.models.load_model('/content/Face_CNN_Model.h5')"
      ],
      "metadata": {
        "id": "XH-J53tOWnP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Below we are taking some sample realtime images for testing the model"
      ],
      "metadata": {
        "id": "B2ZdHpdv_O29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frame = cv2.imread('/content/I3.jpg')  # insert here a live image"
      ],
      "metadata": {
        "id": "mhKp1so-oN7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame.shape"
      ],
      "metadata": {
        "id": "2fkGQqiaoqhv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30ee4392-ddc2-4803-b432-50429e6deae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(720, 1080, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
      ],
      "metadata": {
        "id": "DuRGHfdEosaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Below XML file is used to extract the front part of our facial structure"
      ],
      "metadata": {
        "id": "Ebd0eRYR_YMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
      ],
      "metadata": {
        "id": "EZKXmMQIqo4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)"
      ],
      "metadata": {
        "id": "POdph3q1q634"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Face Detection Logic"
      ],
      "metadata": {
        "id": "KXwt1VUi_mp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
        "for x,y,w,h in faces:\n",
        "  roi_gray = gray[y:y+h, x:x+w]\n",
        "  roi_color = frame[y:y+h, x:x+w]\n",
        "  cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)  # BGR\n",
        "  face_str = faceCascade.detectMultiScale(roi_gray)\n",
        "  if len(face_str) == 0:\n",
        "    print(\"Face not detected\")\n",
        "  else:\n",
        "    for (ex, ey, ew, eh) in face_str:\n",
        "      face_roi = roi_color[ey:ey+eh, ex:ex+ew]"
      ],
      "metadata": {
        "id": "q6JU4skerI09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
      ],
      "metadata": {
        "id": "_objGGJJslIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB))"
      ],
      "metadata": {
        "id": "3iHTshn1sw3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resizing the image size and normalizing the image pixels"
      ],
      "metadata": {
        "id": "vMJ_YyK4_qwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_image = cv2.resize(face_roi, (224, 224))\n",
        "final_image = np.expand_dims(final_image, axis=0) \n",
        "final_image = final_image/255.0   # Normalizing the image"
      ],
      "metadata": {
        "id": "jC_XU240tAyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting the realtime images using our CNN model"
      ],
      "metadata": {
        "id": "6rbqKYg6_zAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = new_model.predict(final_image)"
      ],
      "metadata": {
        "id": "EVMwlGIfug1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29dfdef4-4218-4d9f-f50c-46e56a1ca399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 46ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the predicted scores for each category, the category that ranks with higher score is the identity of that image"
      ],
      "metadata": {
        "id": "m5OCHsl6_79n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[0]"
      ],
      "metadata": {
        "id": "MRHPChHruJ3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "697182b4-87f9-409a-aa66-c57856239b82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7.3814020e-03, 1.5741108e-11, 4.6223597e-08, 8.1112850e-01,\n",
              "       1.7582282e-01, 1.1609286e-04, 5.5510793e-03, 0.0000000e+00,\n",
              "       1.0324916e-35, 1.6907600e-33], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the maximum score's **index**"
      ],
      "metadata": {
        "id": "1MQjL9iWAU9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(predictions)"
      ],
      "metadata": {
        "id": "vUDoKKs_uiXG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae56c7d1-cd46-4f29-d4ff-255579a4e8f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To get the category of image"
      ],
      "metadata": {
        "id": "VxJK7BC7uoTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Facial emotion status of the image : \")\n",
        "classes[np.argmax(predictions)]"
      ],
      "metadata": {
        "id": "E4ftxWsMunis",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "70ae003b-44a9-4796-d91b-b24ce79f07d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Facial emotion status of the image : \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'happy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the model and test the image"
      ],
      "metadata": {
        "id": "uc2bVT3yIzts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data_dir = '/content/Image/'"
      ],
      "metadata": {
        "id": "FMb_aehJkUdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ext = ['png', 'jpeg', 'jpg']    # Add image formats here\n",
        "files = []\n",
        "[files.extend(glob.glob(Data_dir + '*.' + e)) for e in ext]\n",
        "images = [cv2.imread(file) for file in files]\n",
        "print(len(images))"
      ],
      "metadata": {
        "id": "Jr6lXmUdaN13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d9f87c8-0ec1-43a9-ac71-85e7f8969600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gray_image = []\n",
        "for i in range(len(images)):\n",
        "  gray_image.append(cv2.cvtColor(images[i], cv2.COLOR_BGR2GRAY))"
      ],
      "metadata": {
        "id": "2G4F7CCFkcX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def face_detection(frame, gray):\n",
        "  faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
        "  for x,y,w,h in faces:\n",
        "    roi_gray = gray[y:y+h, x:x+w]\n",
        "    roi_color = frame[y:y+h, x:x+w]\n",
        "    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)  # BGR\n",
        "    face_str = faceCascade.detectMultiScale(roi_gray)\n",
        "    if len(face_str) == 0:\n",
        "      print(\"Face not detected\")\n",
        "    else:\n",
        "      for (ex, ey, ew, eh) in face_str:\n",
        "        face_roi = roi_color[ey:ey+eh, ex:ex+ew]\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "i7p2IhUjm57m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_img_array = []\n",
        "for i in range(len(gray_image)):\n",
        "  face_detection(images[i], gray_image[i])\n",
        "  new_img_array.append(images[i])"
      ],
      "metadata": {
        "id": "AICQqAzhnH6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf8ac78-fb67-477f-a2e5-a288fbe7504c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Face not detected\n",
            "Face not detected\n",
            "Face not detected\n",
            "Face not detected\n",
            "Face not detected\n",
            "Face not detected\n",
            "Face not detected\n",
            "Face not detected\n",
            "Face not detected\n",
            "Face not detected\n",
            "Face not detected\n",
            "Face not detected\n",
            "Face not detected\n",
            "Face not detected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(new_img_array)):\n",
        "  plt.imshow(cv2.cvtColor(new_img_array[i], cv2.COLOR_BGR2RGB))"
      ],
      "metadata": {
        "id": "iyhN7HwpnKRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(new_img_array)):\n",
        "    new_img_array[i] = cv2.resize(new_img_array[i], (224, 224))\n",
        "    new_img_array[i] = np.expand_dims(new_img_array[i], axis=0) \n",
        "    new_img_array[i] = new_img_array[i] / 255.0"
      ],
      "metadata": {
        "id": "TWpra2KynRXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_pred = []\n",
        "for i in range(len(new_img_array)):\n",
        "  new_pred.append(new_model.predict(new_img_array[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFjn06tIruY_",
        "outputId": "f9299ebe-24c3-46b1-c272-4666c58561f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 345ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_scores = []\n",
        "for i in range(len(new_img_array)):\n",
        "  pred_scores.append(new_pred[i][0])"
      ],
      "metadata": {
        "id": "OFs5Sd5psqYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_score_index = []\n",
        "for i in range(len(new_img_array)):\n",
        "  pred_score_index.append(np.argmax(pred_scores[i]))"
      ],
      "metadata": {
        "id": "RdAMN6GKtuv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_count = 0\n",
        "pred_sentiment = []\n",
        "for i in range(len(pred_score_index)):\n",
        "  pred_sentiment.append(classes[pred_score_index[i]])"
      ],
      "metadata": {
        "id": "LQtqeb9GuNO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred_sentiment)"
      ],
      "metadata": {
        "id": "1-Ezx2XKv_aT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "542f0dd0-46a5-4c17-9c73-097297ce3921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sad', 'angry', 'fear']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ac = 0  # angry count\n",
        "dc = 0  # disgust count\n",
        "fc = 0  # fear count\n",
        "hc = 0  # happy count\n",
        "nc = 0  # neutral count\n",
        "sac = 0 # sad count\n",
        "suc = 0 # surprise count"
      ],
      "metadata": {
        "id": "i3-jpafYwW4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(pred_sentiment)):\n",
        "  if(pred_sentiment[i] == 'angry'):\n",
        "    ac += 1\n",
        "  elif (pred_sentiment[i] == 'disgust'):\n",
        "    dc += 1\n",
        "  elif (pred_sentiment[i] == 'fear'):\n",
        "    fc += 1\n",
        "  elif (pred_sentiment[i] == 'happy'):\n",
        "    hc += 1\n",
        "  elif (pred_sentiment[i] == 'neutral'):\n",
        "    nc += 1\n",
        "  elif (pred_sentiment[i] == 'sad'):\n",
        "    sac += 1\n",
        "  elif (pred_sentiment[i] == 'surprise'):\n",
        "    suc += 1"
      ],
      "metadata": {
        "id": "FfG9kmuzwxQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Angry :\", ac, \" Disgust :\", dc, \" Fear :\", fc, \" Happy :\", hc, \" Neutral :\", nc, \" Sad :\", sac, \" Surprise :\", suc)"
      ],
      "metadata": {
        "id": "62iB5tEuxAle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6cc2f1-3851-4260-d6d6-56bfba270aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Angry : 12  Disgust : 4  Fear : 4  Happy : 14  Neutral : 15  Sad : 25  Surprise : 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "angry_per =     round(((ac) / (ac + dc + fc + hc + nc + sac + suc)) * 100, 2)\n",
        "disgust_per =   round(((dc) / (ac + dc + fc + hc + nc + sac + suc)) * 100, 2)\n",
        "fear_per =      round(((fc) / (ac + dc + fc + hc + nc + sac + suc)) * 100, 2)\n",
        "happy_per =     round(((hc) / (ac + dc + fc + hc + nc + sac + suc)) * 100, 2)\n",
        "neutral_per =   round(((nc) / (ac + dc + fc + hc + nc + sac + suc)) * 100, 2)\n",
        "sad_per =       round(((sac) / (ac + dc + fc + hc + nc + sac + suc)) * 100, 2)\n",
        "surprise_per =  round(((suc) / (ac + dc + fc + hc + nc + sac + suc)) * 100, 2)\n",
        "print(\"Angry    :\", angry_per, \"%\\nDisgust  :\", disgust_per, \"%\\nFear     :\", fear_per, \"%\\nHappy    :\", happy_per, \"%\\nNeutral  :\", neutral_per, \"%\\nSad      :\", sad_per, \"%\\nSurprise :\", surprise_per, \"%\")"
      ],
      "metadata": {
        "id": "6XqFMwKdBV9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8205ff8-b295-4e22-ed6a-5953fefd8528"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Angry    : 15.58 %\n",
            "Disgust  : 5.19 %\n",
            "Fear     : 5.19 %\n",
            "Happy    : 18.18 %\n",
            "Neutral  : 19.48 %\n",
            "Sad      : 32.47 %\n",
            "Surprise : 3.9 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logic for Depression from CNN model"
      ],
      "metadata": {
        "id": "9xiN-5qc4nu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depression_level = (angry_per + disgust_per + fear_per + sad_per) / 4\n",
        "depression_level"
      ],
      "metadata": {
        "id": "UxwHRVhiXaRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94f4989f-18db-49c3-aef7-86c1418113e0"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14.6075"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if(depression_level >= 1.0 and depression_level <= 4.9):\n",
        "  print(\"Minimal Depression\")\n",
        "elif(depression_level >= 5.0 and depression_level <= 9.9):\n",
        "  print(\"Mild Depression\")\n",
        "elif(depression_level >= 10.0 and depression_level <= 14.9):\n",
        "  print(\"Moderate Depression\")\n",
        "elif(depression_level >= 15.0 and depression_level <= 19.9):\n",
        "  print(\"Moderately Severe Depression\")\n",
        "elif(depression_level >= 20.0):\n",
        "  print(\"Severe Depression\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi2jvytR3EZo",
        "outputId": "a26c02bb-0ff1-4126-9f34-c0aabb975970"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moderate Depression\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}